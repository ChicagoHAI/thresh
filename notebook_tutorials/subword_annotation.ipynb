{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-word level Selection\n",
    "Short demo to show how to set up token boundaries for sub-word annotation selection. See [**nlproc.tools/?t=demo_tokenization**](https://nlproc.tools/?t=demo_tokenization) for more information and an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's declare some data to be tokenized\n",
    "data = [{\n",
    "    \"source\": \"Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles, and, by opposing, end them?\",\n",
    "    \"target\": \"Is it nobler to put up with all the nasty things that luck throws your way, or to fight against all those troubles by simply putting an end to them once and for all?\"\n",
    "}, {\n",
    "    \"source\": \"Alas, poor Yorick! I knew him, Horatio: a fellow of infinite jest, of most excellent fancy: he hath borne me on his back a thousand times; and now, how abhorred in my imagination it is!\",\n",
    "    \"target\": \"Oh, poor Yorick! I used to know him, Horatio—a very funny guy, and with an excellent imagination. He carried me on his back a thousand times, and now—how terrible—this is him.\"\n",
    "}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize our data\n",
    "tokenized = []\n",
    "for sent in data:\n",
    "  sent_pair_tokenized = {}\n",
    "  for sent_type in sent:\n",
    "    sent_tokenized = ' '.join(tokenizer.tokenize(sent[sent_type]))\n",
    "    # sent_tokenized = sent_tokenized.replace(' ', '').replace('Ġ', ' ') # Uncomment to recover the original sentence\n",
    "    sent_pair_tokenized[sent_type] = sent_tokenized\n",
    "  tokenized += [sent_pair_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now our data will work out-of-the-box with the tokenization: tokenized flag!\n",
    "print(tokenized)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
