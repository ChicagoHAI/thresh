{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Fine-Grained Data with `nlproc_tools`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nlproc_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlproc_tools import load_interface, convert_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "When you collect data on nlproc.tools, you will eventually have a large amount of `<<data>>.json` files to manage. \n",
    "\n",
    "Instead of having to create dataloaders to verify each interface (which leads to boilerplate code, and can become quite complex), our `nlproc_tools` library handles all of this for you!\n",
    "\n",
    "This notebook will demonstrate the basic functionality of `load_interface` and `convert_dataset` to manage your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by downloading the SALSA typology and demo dataset\n",
    "!curl -so salsa.yml https://nlproc.tools/templates/salsa.yml\n",
    "!curl -so salsa.json https://nlproc.tools/data/salsa.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💃 SALSA\n",
      "Success and FAilure Linguistic Simplification Annotation\n",
      "@article{heineman2023dancing,\n",
      "  title={Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA},\n",
      "  author={Heineman, David and Dou, Yao and Maddela, Mounica and Xu, Wei},\n",
      "  journal={arXiv preprint arXiv:2305.14458},\n",
      "  year={2023}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# We can load any interface YML file into a Python object\n",
    "SALSA = load_interface(\"salsa.yml\")\n",
    "\n",
    "# Here we can view some features from the interface\n",
    "print(SALSA.template_label)\n",
    "print(SALSA.template_description)\n",
    "print(SALSA.citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 23:38:43,816 - WARNING - Missing key 'insertion_type' from annotation '{'grammar_error': 'no'}'. Keys as defined by the 'insertion' annotation type are: ['insertion_type', 'grammar_error']\n",
      "2023-07-28 23:38:43,817 - WARNING - Missing key 'insertion_type' from annotation '{'grammar_error': 'no'}'. Keys as defined by the 'insertion' annotation type are: ['insertion_type', 'grammar_error']\n",
      "2023-07-28 23:38:43,818 - WARNING - Missing key 'substitution_info_change' from annotation '{'more': {'val': 'elaboration', 'elaboration': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,818 - WARNING - Missing key 'substitution_info_change' from annotation '{'less': {'val': 'good_deletion', 'good_deletion': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,819 - WARNING - Missing key 'insertion_type' from annotation '{'grammar_error': 'no'}'. Keys as defined by the 'insertion' annotation type are: ['insertion_type', 'grammar_error']\n",
      "2023-07-28 23:38:43,820 - WARNING - Missing key 'substitution_info_change' from annotation '{'more': {'val': 'elaboration', 'elaboration': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,821 - WARNING - Missing key 'substitution_info_change' from annotation '{'less': {'val': 'good_deletion', 'good_deletion': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,822 - WARNING - Missing key 'substitution_info_change' from annotation '{'less': {'val': 'good_deletion', 'good_deletion': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,824 - WARNING - Missing key 'substitution_info_change' from annotation '{'more': {'val': 'elaboration', 'elaboration': 'somewhat'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,825 - WARNING - Missing key 'substitution_info_change' from annotation '{'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,825 - WARNING - Missing key 'substitution_info_change' from annotation '{'less': {'val': 'bad_deletion', 'bad_deletion': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,826 - WARNING - Missing key 'substitution_info_change' from annotation '{'less': {'val': 'good_deletion', 'good_deletion': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,826 - WARNING - Missing key 'substitution_info_change' from annotation '{'less': {'val': 'good_deletion', 'good_deletion': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,827 - WARNING - Missing key 'substitution_info_change' from annotation '{'less': {'val': 'good_deletion', 'good_deletion': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,828 - WARNING - Missing key 'substitution_info_change' from annotation '{'less': {'val': 'good_deletion', 'good_deletion': 'somewhat'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,829 - WARNING - Missing key 'substitution_info_change' from annotation '{'more': {'val': 'elaboration', 'elaboration': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,829 - WARNING - Missing key 'substitution_info_change' from annotation '{'less': {'val': 'good_deletion', 'good_deletion': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,830 - WARNING - Missing key 'substitution_info_change' from annotation '{'more': {'val': 'elaboration', 'elaboration': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,831 - WARNING - Missing key 'substitution_info_change' from annotation '{'less': {'val': 'good_deletion', 'good_deletion': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,831 - WARNING - Missing key 'substitution_info_change' from annotation '{'more': {'val': 'elaboration', 'elaboration': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,832 - WARNING - Missing key 'insertion_type' from annotation '{'grammar_error': 'no'}'. Keys as defined by the 'insertion' annotation type are: ['insertion_type', 'grammar_error']\n",
      "2023-07-28 23:38:43,832 - WARNING - Missing key 'substitution_info_change' from annotation '{'more': {'val': 'elaboration', 'elaboration': 'minor'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n",
      "2023-07-28 23:38:43,833 - WARNING - Missing key 'substitution_info_change' from annotation '{'less': {'val': 'bad_deletion', 'bad_deletion': 'somewhat'}, 'grammar_error': 'no'}'. Keys as defined by the 'substitution' annotation type are: ['substitution_info_change', 'grammar_error']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SalsaEntry(\n",
      "  annotator: annotator_1, \n",
      "  system: new-wiki-1/GPT-3-zero-shot, \n",
      "  source: The film has grossed over $552 million worldwide, becoming the eighth highest-grossing film of 2022., \n",
      "  target: The film has made more than $552 million at the box office and is currently the eighth most successful movie of 2022., \n",
      "  edits: [DeletionEdit(\n",
      "    input_idx: [[39, 49]], \n",
      "    annotation: DeletionAnnotation(\n",
      "      deletion_type: GoodDeletion(\n",
      "        val: 1\n",
      "      ), \n",
      "      coreference: False, \n",
      "      grammar_error: False\n",
      "    )\n",
      "  ), SubstitutionEdit(\n",
      "    input_idx: [[13, 20]], \n",
      "    output_idx: [[13, 17]], \n",
      "    annotation: SubstitutionAnnotation(\n",
      "      substitution_info_change: Same(\n",
      "        val: 2\n",
      "      ), \n",
      "      grammar_error: False\n",
      "    )\n",
      "  ), SubstitutionEdit(\n",
      "    input_idx: [[21, 25]], \n",
      "    output_idx: [[18, 27]], \n",
      "    annotation: SubstitutionAnnotation(\n",
      "      substitution_info_change: Same(\n",
      "        val: 1\n",
      "      ), \n",
      "      grammar_error: False\n",
      "    )\n",
      "  ), SubstitutionEdit(\n",
      "    input_idx: [[70, 86]], \n",
      "    output_idx: [[87, 102]], \n",
      "    annotation: SubstitutionAnnotation(\n",
      "      substitution_info_change: Same(\n",
      "        val: 2\n",
      "      ), \n",
      "      grammar_error: False\n",
      "    )\n",
      "  ), SubstitutionEdit(\n",
      "    input_idx: [[87, 91]], \n",
      "    output_idx: [[103, 108]], \n",
      "    annotation: SubstitutionAnnotation(\n",
      "      substitution_info_change: Same(\n",
      "        val: trivial\n",
      "      ), \n",
      "      grammar_error: False\n",
      "    )\n",
      "  ), InsertionEdit(\n",
      "    output_idx: [[41, 58]], \n",
      "    annotation: InsertionAnnotation(\n",
      "      insertion_type: Elaboration(\n",
      "        val: 1\n",
      "      ), \n",
      "      grammar_error: False\n",
      "    )\n",
      "  ), StructureEdit(\n",
      "    constituent_edits: [SubstitutionEdit(\n",
      "      id: 5, \n",
      "      category: substitution, \n",
      "      input_idx: [[50, 58]], \n",
      "      output_idx: [[59, 75]]\n",
      "    )], \n",
      "    annotation: StructureAnnotation(\n",
      "      structure_type: Clausal(\n",
      "        val: clausal\n",
      "      ), \n",
      "      impact: Good(\n",
      "        val: 1\n",
      "      ), \n",
      "      grammar_error: False\n",
      "    )\n",
      "  )]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# We can load our JSON data by using our interface object and calling load_annotations()\n",
    "salsa_data = SALSA.load_annotations(\"salsa.json\")\n",
    "\n",
    "# As we can see, it creates a custom object, with custom Edit and Annotation classes according\n",
    "# to the SALSA typology\n",
    "print(salsa_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new-wiki-1/GPT-3-zero-shot\n",
      "annotator_1\n",
      "DeletionEdit(\n",
      "  input_idx: [[39, 49]], \n",
      "  annotation: DeletionAnnotation(\n",
      "    deletion_type: GoodDeletion(\n",
      "      val: 1\n",
      "    ), \n",
      "    coreference: False, \n",
      "    grammar_error: False\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# We can view attributes of an entry via direct calls\n",
    "salsa_entry = salsa_data[0]\n",
    "\n",
    "print(salsa_entry.system)\n",
    "print(salsa_entry.annotator)\n",
    "print(salsa_entry.edits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we use our interface to create our own dataset, we can export it back to JSON\n",
    "SALSA.export_data(salsa_data, \"salsa_export.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal Data Classes\n",
    "\n",
    "The nlproc_library has four classes: `Interface`, `Entry`, `Edit` and `Annotation`\n",
    "\n",
    "In this section, we will show how to use the Edit and Annotation classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SalsaEntry(\n",
      "  system: custom_entry, \n",
      "  annotator: None, \n",
      "  source: This is our complex sentence., \n",
      "  target: This is our complex sentence.\n",
      ")\n",
      "This is our complex sentence.\n"
     ]
    }
   ],
   "source": [
    "# Get the Entry class defined by SALSA\n",
    "SalsaEntry = SALSA.get_entry_class()\n",
    "\n",
    "# Create a new Entry object\n",
    "sent = SalsaEntry(\n",
    "    system=\"custom_entry\",\n",
    "    annotator=None,\n",
    "    source=\"This is our complex sentence.\",\n",
    "    target=\"This is our complex sentence.\"\n",
    ")\n",
    "\n",
    "print(sent)\n",
    "\n",
    "# We can get individual entries by directly calling attributes\n",
    "print(sent.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reorder': nlproc_tools.names.ReorderEdit,\n",
       " 'insertion': nlproc_tools.names.InsertionEdit,\n",
       " 'deletion': nlproc_tools.names.DeletionEdit,\n",
       " 'substitution': nlproc_tools.names.SubstitutionEdit,\n",
       " 'structure': nlproc_tools.names.StructureEdit,\n",
       " 'split': nlproc_tools.names.SplitEdit}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the classes for each Edit\n",
    "SALSA.edit_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InsertionEdit(\n",
      "  output_idx: [[39, 40]]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# We can use get_edit_class() to get a specific Edit class\n",
    "InsertionEdit = SALSA.get_edit_class('insertion')\n",
    "\n",
    "# Then, we can define a new Insertion edit\n",
    "edit = InsertionEdit(\n",
    "    output_idx=[[39, 40]]\n",
    ")\n",
    "\n",
    "print(edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SalsaEntry(\n",
      "  system: custom_entry, \n",
      "  annotator: None, \n",
      "  source: This is our complex sentence., \n",
      "  target: This is our complex sentence., \n",
      "  edits: [SubstitutionEdit(\n",
      "    input_idx: [[1, 4]], \n",
      "    output_idx: [[8, 12]]\n",
      "  ), DeletionEdit(\n",
      "    input_idx: [[1, 5]]\n",
      "  )]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Let's add some edits to our sentence\n",
    "sent.edits = []\n",
    "\n",
    "DeletionEdit = SALSA.get_edit_class('deletion')\n",
    "SubstitutionEdit = SALSA.get_edit_class('substitution')\n",
    "\n",
    "sent.edits.append(\n",
    "    SubstitutionEdit(\n",
    "        input_idx=[[1, 4]],\n",
    "        output_idx=[[8, 12]],\n",
    "    )\n",
    ")\n",
    "\n",
    "sent.edits.append(\n",
    "    DeletionEdit(\n",
    "        input_idx=[[1, 5]],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# We can now use our typology to convert our data back to a raw format\n",
    "raw_salsa_data = SALSA.to_dict([sent])\n",
    "print(raw_salsa_data)\n",
    "\n",
    "# Feel free to paste this into nlproc.tools/?t=salsa to see a vizualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One side note: If you want to load, you can simply add the datasets:\n",
    "# SALSA.load_annotations(\"dataset_1.json\") + SALSA.load_annotations(\"dataset_2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Conversion\n",
    "Our standardized nlproc.tools data format is universal across fine-grained annotation schemes. To support exisitng data formats, we created dataloaders from external datasets.\n",
    "\n",
    "We support the following datasets:\n",
    "```\n",
    "frank, scarecrow, mqm, snac, wu-etal-2023, da-san-martino-etal-2019\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 23:38:50,051 - INFO - ============================================================\n",
      "2023-07-28 23:38:50,052 - INFO - Dataset name: snac\n",
      "2023-07-28 23:38:50,053 - INFO - Data path: original_snac_data.json\n",
      "2023-07-28 23:38:50,053 - INFO - Reverse flag: False\n",
      "2023-07-28 23:38:50,054 - INFO - Output path (Optional): None\n",
      "2023-07-28 23:38:50,055 - INFO - ============================================================\n",
      "2023-07-28 23:38:50,056 - INFO - Converting dataset...\n",
      "2023-07-28 23:38:50,084 - INFO - Done!\n"
     ]
    }
   ],
   "source": [
    "# First, we'll download the SNaC dataset in its original form\n",
    "!pip install -q gdown\n",
    "import gdown\n",
    "gdown.download(f'https://drive.google.com/uc?id=1ff-pV2sX9XNDMdaPxY7v22T2i0235tcE', 'original_snac_data.json', quiet=True)\n",
    "\n",
    "# Then, we will convert it to our standard format\n",
    "snac_raw = convert_dataset(\n",
    "    dataset_name=\"snac\",\n",
    "    data_path=\"original_snac_data.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '',\n",
       " 'target': 'Johnnie, a determined 19 year old girl, works at the cotton mill to support her family. She plans to pay back all the debts they have accumulated.',\n",
       " 'edits': [{'id': 0, 'category': 'CharE', 'output_idx': [[0, 7]], 'votes': 1}]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data conversion is now in the JSON format, let's look at an example\n",
    "snac_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SnacEntry(\n",
       "  source: , \n",
       "  target: Johnnie, a determined 19 year old girl, works at the cotton mill to support her family. She plans to pay back all the debts they have accumulated., \n",
       "  edits: [ChareEdit(\n",
       "    output_idx: [[0, 7]], \n",
       "    votes: 1\n",
       "  )]\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To begin using this, we'll download and use the nlproc.tools SNaC typology\n",
    "!curl -so snac.yml https://nlproc.tools/templates/snac.yml\n",
    "\n",
    "# Now we can load and view in the standardized nlproc.tools format!\n",
    "SNaC = load_interface(\"snac.yml\")\n",
    "snac = SNaC.load_annotations(snac_raw)\n",
    "\n",
    "snac[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples: Loading Demo Interface Data\n",
    "Now that we've seen our data loading and dataset conversion, let's see some examples across different tasks.\n",
    "\n",
    "These cells will load our demo data for each interface as it is seen in nlproc.tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultipitEntry(\n",
       "  source: Relax, take it easy., \n",
       "  target: Relax, take a deep breath, and enjoy the moment., \n",
       "  edits: [AddNewEdit(\n",
       "    annotation: None, \n",
       "    output_idx: [[14, 48]]\n",
       "  )]\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!curl -so multipit.yml https://nlproc.tools/templates/multipit.yml\n",
    "!curl -so multipit.json https://nlproc.tools/data/multipit.json\n",
    "\n",
    "MultiPIT = load_interface(\"multipit.yml\")\n",
    "multipit_data = MultiPIT.load_annotations(\"multipit.json\")\n",
    "\n",
    "multipit_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'OtherE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-16220e947b77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mFRANK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_interface\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"frank.yml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfrank_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFRANK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_annotations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"frank.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfrank_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\nlproc_tools\\names.py\u001b[0m in \u001b[0;36mload_annotations\u001b[1;34m(self, data_or_filename)\u001b[0m\n\u001b[0;32m    118\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;34m'category'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                     \u001b[0mEditClass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medit_classes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0medit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m                     \u001b[0mnlproc_edit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEditClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexcluded_edit_cols\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                     \u001b[0mnlproc_edits\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnlproc_edit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'OtherE'"
     ]
    }
   ],
   "source": [
    "!curl -so frank.yml https://nlproc.tools/templates/frank.yml\n",
    "!curl -so frank.json https://nlproc.tools/data/frank.json\n",
    "\n",
    "FRANK = load_interface(\"frank.yml\")\n",
    "frank_data = FRANK.load_annotations(\"frank.json\")\n",
    "\n",
    "frank_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CwzccEntry(\n",
       "  source: , \n",
       "  target: Pirmi man iyo ta sinti dwele. Nusabe yo porke pirmi ansina. Kwando iyo keda alegre? Kwando gaha pasa el diya o mes na iyo triste?, \n",
       "  edits: [UnintentionalEdit(\n",
       "    annotation: UnintentionalAnnotation(\n",
       "      val: None, \n",
       "      error_type: NonRandomErrors(\n",
       "        val: ArbitrarySpelling(\n",
       "          val: Phonogramical(\n",
       "            val: None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ), \n",
       "    output_idx: [[0, 5]]\n",
       "  ), UnintentionalEdit(\n",
       "    annotation: UnintentionalAnnotation(\n",
       "      val: None, \n",
       "      error_type: NonRandomErrors(\n",
       "        val: ArbitrarySpelling(\n",
       "          val: Phonogramical(\n",
       "            val: None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ), \n",
       "    output_idx: [[10, 13]]\n",
       "  ), UnintentionalEdit(\n",
       "    annotation: UnintentionalAnnotation(\n",
       "      val: None, \n",
       "      error_type: NonRandomErrors(\n",
       "        val: ArbitrarySpelling(\n",
       "          val: Phonogramical(\n",
       "            val: None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ), \n",
       "    output_idx: [[17, 22]]\n",
       "  ), UnintentionalEdit(\n",
       "    annotation: UnintentionalAnnotation(\n",
       "      val: None, \n",
       "      error_type: NonRandomErrors(\n",
       "        val: ArbitrarySpelling(\n",
       "          val: Phonogramical(\n",
       "            val: None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ), \n",
       "    output_idx: [[23, 28]]\n",
       "  ), UnintentionalEdit(\n",
       "    annotation: UnintentionalAnnotation(\n",
       "      val: None, \n",
       "      error_type: NonRandomErrors(\n",
       "        val: RegularError(\n",
       "          val: Segmentation(\n",
       "            val: None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ), \n",
       "    output_idx: [[30, 36]]\n",
       "  ), UnintentionalEdit(\n",
       "    annotation: UnintentionalAnnotation(\n",
       "      val: None, \n",
       "      error_type: NonRandomErrors(\n",
       "        val: ArbitrarySpelling(\n",
       "          val: Phonogramical(\n",
       "            val: None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ), \n",
       "    output_idx: [[40, 45]]\n",
       "  ), UnintentionalEdit(\n",
       "    annotation: UnintentionalAnnotation(\n",
       "      val: None, \n",
       "      error_type: NonRandomErrors(\n",
       "        val: ArbitrarySpelling(\n",
       "          val: Phonogramical(\n",
       "            val: None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ), \n",
       "    output_idx: [[46, 51]]\n",
       "  ), UnintentionalEdit(\n",
       "    annotation: UnintentionalAnnotation(\n",
       "      val: None, \n",
       "      error_type: NonRandomErrors(\n",
       "        val: ArbitrarySpelling(\n",
       "          val: Phonogramical(\n",
       "            val: None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ), \n",
       "    output_idx: [[60, 66]]\n",
       "  ), UnintentionalEdit(\n",
       "    annotation: UnintentionalAnnotation(\n",
       "      val: None, \n",
       "      error_type: NonRandomErrors(\n",
       "        val: ArbitrarySpelling(\n",
       "          val: Phonogramical(\n",
       "            val: None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ), \n",
       "    output_idx: [[67, 70]]\n",
       "  ), UnintentionalEdit(\n",
       "    annotation: UnintentionalAnnotation(\n",
       "      val: None, \n",
       "      error_type: NonRandomErrors(\n",
       "        val: ArbitrarySpelling(\n",
       "          val: Phonogramical(\n",
       "            val: None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ), \n",
       "    output_idx: [[71, 75]]\n",
       "  ), UnintentionalEdit(\n",
       "    annotation: UnintentionalAnnotation(\n",
       "      val: None, \n",
       "      error_type: NonRandomErrors(\n",
       "        val: ArbitrarySpelling(\n",
       "          val: Phonogramical(\n",
       "            val: None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ), \n",
       "    output_idx: [[84, 90]]\n",
       "  ), UnintentionalEdit(\n",
       "    annotation: UnintentionalAnnotation(\n",
       "      val: None, \n",
       "      error_type: NonRandomErrors(\n",
       "        val: ArbitrarySpelling(\n",
       "          val: Phonogramical(\n",
       "            val: None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ), \n",
       "    output_idx: [[104, 108]]\n",
       "  ), UnintentionalEdit(\n",
       "    annotation: UnintentionalAnnotation(\n",
       "      val: None, \n",
       "      error_type: NonRandomErrors(\n",
       "        val: ArbitrarySpelling(\n",
       "          val: Phonogramical(\n",
       "            val: None\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ), \n",
       "    output_idx: [[118, 121]]\n",
       "  )]\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!curl -so cwzcc.yml https://nlproc.tools/templates/cwzcc.yml\n",
    "!curl -so cwzcc.json https://nlproc.tools/data/cwzcc.json\n",
    "\n",
    "CWZCC = load_interface(\"cwzcc.yml\")\n",
    "cwzcc_data = CWZCC.load_annotations(\"cwzcc.json\")\n",
    "\n",
    "cwzcc_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScarecrowEntry(\n",
       "  model: gpt3, \n",
       "  frequency_penalty: 0, \n",
       "  temperature: 1.0, \n",
       "  p: 0.96, \n",
       "  gid: 10001, \n",
       "  id: 1, \n",
       "  source: In the wild, animals display tender moments of affection all the time. Macedonian photographer Goran Anastasovski has spent 15 years honing his skills in wildlife photography so that he can share these touching instances with others., \n",
       "  target: To honor the effort he put into his latest set of photographs, we've gathered some of our favorites for your viewing pleasure. A lion and its cub enjoy a tender moment together. The lion's paws rest on top of the front paws of its cub, who lays its head on its mother. A polar bear embraces her cub. Polar bears care for their cubs until they're about two years old, when the cubs venture out on their own. A kangaroo rubs her cheek on her joey's hand., \n",
       "  edits: [NeedsGoogleEdit(\n",
       "    output_idx: [[300, 366]], \n",
       "    annotation: NeedsGoogleAnnotation(\n",
       "      explaination: I'm not sure if polar bear cubs stay with their mothers until they are two years old I would need to use google to find out. , \n",
       "      needs_google_type: 1\n",
       "    )\n",
       "  ), GrammarUsageEdit(\n",
       "    output_idx: [[236, 239]], \n",
       "    annotation: GrammarUsageAnnotation(\n",
       "      explaination: This \"who\" isn't very clear, it should be something like \"while the cub\", \n",
       "      grammar_usage_type: 2\n",
       "    )\n",
       "  )]\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!curl -so scarecrow.yml https://nlproc.tools/templates/scarecrow.yml\n",
    "!curl -so scarecrow.json https://nlproc.tools/data/scarecrow.json\n",
    "\n",
    "SCARECROW = load_interface(\"scarecrow.yml\")\n",
    "scarecrow_data = SCARECROW.load_annotations(\"scarecrow.json\")\n",
    "\n",
    "scarecrow_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SnacEntry(\n",
       "  source: , \n",
       "  target: It is also said that seeing Anytus pass by, Socrates remarked: \"How proudly the great man steps; he thinks, no doubt, he has performed some great and noble deed in putting me to death, and all because, seeing him deemed worthy of the highest honours of the state, I told him it ill became him to bring up his so in a tan-yard.\" He adds that Homer has ascribed to some at the point of death a power of forecasting things, and he too is minded to utter a prophecy. Once, for a brief space, he associated with the son of Anytus, and he seemed, \n",
       "  edits: [SceneeEdit(\n",
       "    output_idx: [[0, 42]], \n",
       "    votes: 1\n",
       "  ), GrameEdit(\n",
       "    output_idx: [[264, 326]], \n",
       "    votes: 1\n",
       "  ), ChareEdit(\n",
       "    output_idx: [[28, 34]], \n",
       "    votes: 1\n",
       "  ), ChareEdit(\n",
       "    output_idx: [[341, 346]], \n",
       "    votes: 1\n",
       "  ), GrameEdit(\n",
       "    output_idx: [[526, 539]], \n",
       "    votes: 2\n",
       "  )]\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!curl -so snac.yml https://nlproc.tools/templates/snac.yml\n",
    "!curl -so snac.json https://nlproc.tools/data/snac.json\n",
    "\n",
    "SNaC = load_interface(\"snac.yml\")\n",
    "snac_data = SNaC.load_annotations(\"snac.json\")\n",
    "\n",
    "snac_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Da-san-martino-etal-2019Entry(\n",
       "  source: , \n",
       "  target: The US Is Blatantly Telling Lies\n",
       "  \n",
       "  It’s no secret that the Trump administration has a strong distaste for Iran.\n",
       "  Iran is one of the only issues on which the U.S. president has remained relatively consistent.\n",
       "  Trump berated the country both before and after taking office.\n",
       "  However, Trump’s anti-Iran strategy goes against the better judgment of even the most anti-Iranian advisors in his staff who don’t want to see the U.S. isolated on the world stage.\n",
       "  Fortunately for Trump, however, he is not alone in his bid to isolate and demonize Iran at all costs.\n",
       "  On December 12, Trump’s ambassador to the U.N., Nikki Haley, gave a grandiose speech demonizing Iran that echoed Colin Powell’s infamous performance before the U.N. in 2003.\n",
       "  Haley’s essential claim was that Saudi Arabia is under attack by missiles supplied to Yemen by the Iranian government and that the world should not sit idly by as this goes on.\n",
       "  “If we do nothing about the missiles fired at Saudi Arabia, we will not be able to stop the violence,” Haley warned.\n",
       "  “There is clear evidence that the missiles that landed on Saudi Arabia come from Iran,” she said.\n",
       "  “The evidence is undeniable.\n",
       "  The weapons might as well have had ‘Made in Iran’ stickers all over it.”\n",
       "  Buy Gold at Discounted Prices\n",
       "  However, even as Haley opened her mouth, many commentators could already identify a number of issues with her speech.\n",
       "  As Common Dream’s Reza Marashi explained:\n",
       "  “Haley cited a UN report in her claim regarding Iranian missile transfers to the Houthis.\n",
       "  Of course, the UN has reached no such conclusion.\n",
       "  Instead, a panel of experts concluded that fired missile fragments show components from an Iranian company, but they have ‘no evidence as to the identity of the broker or supplier.’ Asked about Haley’s claim that Iran is the culprit, Sweden’s ambassador to the UN said, ‘The info I have is less clear.’ Analysts from the U.S. Department of Defense speaking to reporters at Haley’s speech openly acknowledged that they do not know the missiles’ origin.\n",
       "  Perhaps most surreal is the very same UN report cited by Haley also says the missile included a component that was manufactured by an American company.\n",
       "  Did she disingenuously omit that inconvenient bit from her remarks, or fail to read the entire UN report?\n",
       "  The world may never know.”\n",
       "  Regardless of the fact that Haley misrepresented the U.N. report in question, it appears the entire premise of the U.N. report is almost completely incorrect, anyway, according to former inspector Scott Ritter.\n",
       "  Ritter writes:\n",
       "  “The missile debris in question actually contradicts the finding of the UN panel, which held that the missiles launched against Saudi Arabia had been transferred to Yemen in pieces and assembled there by Houthi missile engineers; it is clear that the missiles in question had been in the possession of Yemen well before the Saudi Arabian-led intervention of 2015, and that their source was either Soviet or North Korean.\n",
       "  The modification kits, on the other hand, appear to be of Iranian origin, and were transported to Yemen via Oman.\n",
       "  The UN panel claims not to have any evidence of ‘external missile specialists’ working alongside the Houthi; indeed, the simplicity of the Burkhan 2-H modification concept is such that anyone already familiar with the SCUD-B missile system would be able to implement the required processes without outside assistance.” [emphasis added]\n",
       "  So where did the missiles come from, and who made them?\n",
       "  According to Ritter:\n",
       "  “Rather than the Iranian-manufactured Qiam-1 missiles Haley and the Saudi Arabian government claimed, the debris presented by Haley were of a modified Soviet-manufactured SCUD-B missile; the airframe and engine are original Soviet-made components, and many of the smaller parts on display bear Cyrillic (i.e., Russian) markings.\n",
       "  The transformation to the Burkhan 2-H design required the Houthi engineers to increase the size of the fuel and oxidizer tanks, and lengthen the airframe accordingly.\n",
       "  This is done by cutting the airframe, and welding in place the appropriate segments (this also required that the fuel supply pipe, which passes through the oxidizer tank, be similarly lengthened.)\n",
       "  The difference in quality between the factory welds and the new welds is readily discernable.\n",
       "  The increased fuel supply permits a longer engine burn, which in turn increases the range of the missile.\n",
       "  The Burkhan 2-H uses a smaller warhead than the SCUD B; as such, the guidance and control section had been reconfigured to a smaller diameter, and an inter-stage section added to connect the warhead/guidance section with the main airframe.”\n",
       "  ﻿\n",
       "  Those who have been paying attention to this conflict have been well aware that the U.S. has had little material evidence to link Yemen’s Houthis to Iranian arms suppliers.\n",
       "  In January of this year, a panel of U.N. experts stated:\n",
       "  “The panel has not seen sufficient evidence to confirm any direct large-scale supply of arms from the Government of the Islamic Republic of Iran, although there are indicators that anti-tank guided weapons being supplied to the Houthi or Saleh forces are of Iranian manufacture.” [emphasis added]\n",
       "  Even if Iran were arming the Houthis, Haley’s hypocritical anti-Iran rhetoric doesn’t excuse the U.S. for continuing a foreign policy that essentially armed ISIS through U.S. weapons transfers, or for arming al-Qaeda’s affiliate in Syria, just to name two examples of Washington’s schizophrenic approach to the region.\n",
       "  Why is the U.S. singling out Iran, especially when the Houthi rebels are sworn enemies of al-Qaeda?\n",
       "  The issues here go much deeper than nonsensical hypocrisy.\n",
       "  According to Ritter, the entire debacle has shown that if Saudi Arabia cannot contain the Houthi’s missile capabilities, it cannot possibly hope to take on Iran, which possesses a significantly more advanced military than the Houthis do on their own.\n",
       "  Ritter explains further:\n",
       "  “If a relatively unsophisticated foe such as the Houthi, using Iranian-modified Soviet and North Korean missiles derived from 40-year-old technology, can evade an enemy force using the most modern combat aircraft backed up by the most sophisticated intelligence gathering systems available, and successfully launch ballistic missiles that threaten the political and economic infrastructure of the targeted state, what does that say about the prospects of any U.S.-led coalition taking on the far more advanced mobile missile threats that exist in North Korea and Iran today?\n",
       "  The fact of the matter is that no military anywhere has shown the ability to successfully interdict in any meaningful way a determined opponent armed with mobile ballistic missile capability.\n",
       "  If the Saudi experience in Yemen is to teach us anything, it is that any military plan designed to confront nations such as North Korea, Iran and Russia that are armed with sophisticated mobile ballistic missiles had better count on those capabilities remaining intact throughout any anticipated period of hostility.\n",
       "  No amount of chest-thumping and empty rhetoric by American political and/or military leaders can offset this harsh reality.\n",
       "  This is the critical lesson of Yemen, and the United States would do well to heed it before it tries to foment a crisis based upon trumped-up charges.” [emphasis added]\n",
       "  \n",
       "  , \n",
       "  edits: [AppealToFearPrejudiceEdit(\n",
       "    output_idx: [[904, 1003]]\n",
       "  ), DoubtEdit(\n",
       "    output_idx: [[2154, 2260]]\n",
       "  ), LoadedLanguageEdit(\n",
       "    output_idx: [[5170, 5182]]\n",
       "  ), DoubtEdit(\n",
       "    output_idx: [[0, 32]]\n",
       "  ), DoubtEdit(\n",
       "    output_idx: [[2380, 2444]]\n",
       "  ), AppealToFearPrejudiceEdit(\n",
       "    output_idx: [[7086, 7235]]\n",
       "  ), LoadedLanguageEdit(\n",
       "    output_idx: [[1183, 1219]]\n",
       "  ), DoubtEdit(\n",
       "    output_idx: [[5443, 5543]]\n",
       "  ), WhataboutismEdit(\n",
       "    output_idx: [[305, 337]]\n",
       "  ), AppealToFearPrejudiceEdit(\n",
       "    output_idx: [[355, 449]]\n",
       "  ), LoadedLanguageEdit(\n",
       "    output_idx: [[5405, 5427]]\n",
       "  ), LoadedLanguageEdit(\n",
       "    output_idx: [[6962, 7085]]\n",
       "  )]\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!curl -so da-san-martino-etal-2019.yml https://nlproc.tools/templates/da-san-martino-etal-2019.yml\n",
    "!curl -so da-san-martino-etal-2019.json https://nlproc.tools/data/da-san-martino-etal-2019.json\n",
    "\n",
    "Da_san_martino_etal_2019 = load_interface(\"da-san-martino-etal-2019.yml\")\n",
    "da_san_martino_etal_2019 = Da_san_martino_etal_2019.load_annotations(\"da-san-martino-etal-2019.json\")\n",
    "\n",
    "da_san_martino_etal_2019[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wu-etal-2023Entry(\n",
       "  source: Bloom is the second studio album by Australian singer and songwriter Troye Sivan. This album was released on August 31, 2018 through EMI Music Australia and Capitol Records. The album's title track Bloom, a song about anal sex, was promoted using the hashtag ‘#BopsBoutBottoming, which was trending on Twitter. The song Bloom was released on May 2, 2018 as the third single from Sivan's album Bloom. , \n",
       "  target: Bloom is the second studio album by Australian singer and songwriter Troye Sivan, released on 31 August 2018 through EMI Australia and Capitol Records. The album follows up his 2015 debut studio album, Blue Neighbourhood, and features guest appearances from Gordi and Ariana Grande. The lead single from the album was released on 10 January 2018, accompanied by a music video directed by Grant Singer. The title track, a song about queer desire, was released on 2 May as the third single. American singer Ariana Grande was released on 13 June as the album's fourth single., \n",
       "  context: \n",
       "  \n",
       "  ### Passage Title: Bloom (Troye Sivan album)\n",
       "  Bloom is the second studio album by Australian singer and songwriter Troye Sivan, released on 31 August 2018 through EMI Australia and Capitol Records.\n",
       "  The album follows up his 2015 debut studio album, \"Blue Neighbourhood\", and features guest appearances from Gordi and Ariana Grande.\n",
       "  It was preceded by the release of the singles \"My My My! \", \"The Good Side\", \"Bloom\", \"Dance to This\" and \"Animal\".\n",
       "  At the ARIA Music Awards of 2018, the album was nominated for three awards; Album of the Year, Best Male Artist and Best Pop Release.\n",
       "  \"Bloom\" has been called Sivan's \"sex album\", as well as \"darker\", \"more guitar-driven\" and \"more danceable\" than his previous material.\n",
       "  It has also been described as containing material about defiant gay expression; the first song, \"Seventeen\", is about a sexual experience Sivan had with a man he met on Grindr.\n",
       "  Sivan wrote most of the album with American musician Leland and Canadian musician Allie X.\n",
       "  The album's production was primarily handled by Bram Inscore, Oscar Görres, Oscar Holter and Ariel Rechtshaid.\n",
       "  \"My My My!\"\n",
       "  \n",
       "  ### Passage Title: Bloom (Troye Sivan album)\n",
       "  was released as the lead single from the album on 10 January 2018, and was accompanied by a music video directed by Grant Singer.\n",
       "  The second single \"The Good Side\", was released nine days later.\n",
       "  It is an acoustic track about a breakup, with Sivan explaining that the song is an open letter to an ex-boyfriend.\n",
       "  The title track \"Bloom\", a song about queer desire, was released on 2 May as the third single.\n",
       "  \"Dance to This\", featuring American singer Ariana Grande, was released on 13 June as the album's fourth single.\n",
       "  The fifth single, \"Animal\", was released on 9 August.\n",
       "  On 14 June, Sivan announced a Target special edition would be released featuring two new songs, \"This This\" and \"Running Shoes\".\n",
       "  The album was released on 31 August 2018 by EMI Music Australia and Capitol Records.\n",
       "  Sivan is promoting the album via the Bloom Tour, which began on 21 September in Irving, Texas.\n",
       "  At Metacritic, which assigns a normalised rating out of 100 to reviews from mainstream publications, \"Bloom\" received an average score of 85, based on 15 reviews, indicating \"universal acclaim\".\n",
       "  Giving the album a perfect score, \"The Independent\"s Douglas Greenwood wrote, \"Making perfect pop isn't easy, but Troye Sivan is a star who's done his homework.\n",
       "  \n",
       "  ### Question: When does bloom by troye sivan come out?, \n",
       "  edits: [FactualEdit(\n",
       "    annotation: FactualAnnotation(\n",
       "      factual_error: Irrelevant(\n",
       "        val: irrelevant\n",
       "      ), \n",
       "      explaination: None\n",
       "    ), \n",
       "    output_idx: [[283, 401]]\n",
       "  ), FactualEdit(\n",
       "    annotation: FactualAnnotation(\n",
       "      factual_error: Irrelevant(\n",
       "        val: irrelevant\n",
       "      ), \n",
       "      explaination: None\n",
       "    ), \n",
       "    output_idx: [[489, 572]]\n",
       "  ), FactualEdit(\n",
       "    annotation: FactualAnnotation(\n",
       "      factual_error: Irrelevant(\n",
       "        val: irrelevant\n",
       "      ), \n",
       "      explaination: None\n",
       "    ), \n",
       "    output_idx: [[152, 282]]\n",
       "  )]\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!curl -so wu-etal-2023.yml https://nlproc.tools/templates/wu-etal-2023.yml\n",
    "!curl -so wu-etal-2023.json https://nlproc.tools/data/wu-etal-2023.json\n",
    "\n",
    "Wuetal2023 = load_interface(\"wu-etal-2023.yml\")\n",
    "wu_etal_2023_data = Wuetal2023.load_annotations(\"wu-etal-2023.json\")\n",
    "\n",
    "wu_etal_2023_data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
