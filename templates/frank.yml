template_name: frank
template_label: FRANK
template_description: Factuality in Abstractive Summarization
input_label: "Article"
output_label: "Generated Summary"
edits:
  - name: RelE
    label: "Relation Error"
    type: primitive
    color: red
    icon: 
    enable_output: true
  - name: EntE
    label: "Entity Error"
    type: primitive
    color: blue
    icon: 
    enable_output: true
  - name: CircE
    label: "Circumstance Error"
    type: primitive
    color: teal
    icon: 
    enable_output: true
  - name: CorefE
    label: "Coreference Error"
    type: primitive
    color: yellow
    icon: 
    enable_output: true
  - name: LinkE
    label: "Discourse Link Error"
    type: primitive
    color: orange
    icon: 
    enable_output: true
  - name: OutE
    label: "Out of Article Error"
    type: primitive
    color: green
    icon: 
    enable_output: true
  - name: GramE
    label: "Grammatical Error"
    type: primitive
    color: red
    icon: 
    enable_output: true
citation: |
  @inproceedings{pagnoni-etal-2021-understanding,
    title = "Understanding Factuality in Abstractive Summarization with {FRANK}: A Benchmark for Factuality Metrics",
    author = "Pagnoni, Artidoro  and
      Balachandran, Vidhisha  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.383",
    doi = "10.18653/v1/2021.naacl-main.383",
    pages = "4812--4829",
    abstract = "Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses.",
  }